{
    "QA_PATH": "qa.json",
    "MEMORY_KEY": "chat_history",
    "web": [
        {
            "url": [
                "https://www.marshmallow.com/insurance-for-expats"
            ],
            "name": "new_to_uk"
        },
        {
            "url": [
                "https://www.marshmallow.com/car-insurance",
                "https://www.marshmallow.com/blog/what-is-fully-comprehensive-car-insurance"
            ],
            "name": "car_insurance"
        },
        {
            "url": [
                "https://www.marshmallow.com/van-insurance"
            ],
            "name": "van_insurance"
        },
        {
            "url": [
                "https://www.marshmallow.com/claims"
            ],
            "name": "claims"
        },
        {
            "url": [
                "https://www.marshmallow.com/our-story"
            ],
            "name": "our_story"
        },
        {
            "url": [
                "https://www.marshmallow.com/refer-a-friend"
            ],
            "name": "refer_a_friend"
        }
    ],
    "AGENT_GREETING": "Hi, I'm Marshmallow's Client Service Trainer. I will ask you a few questions.",
    "AGENT_TEMPLATE": "You are a very powerful assistant to give questions and evaluate the answers.\n\nIf the chat history is empty, or the input is empty, you must use the retrieval tool to get a question from the database. The format of the answer: <question to be asked> Answer: <answer to the question>\nIf you see \"Thank you for answering all the questions. Please check your evaluation score\", it means you have asked all the questions in the database. First use the evaluation tool to evaluate the answer of the human, and then use the summary tool to return a summary of the conversation by inputting the chat_history.\nThe format of the answer is: <evaluation of the previous answer>. <summary of the whole conversation>\n\nAfter you have asked your first question, and the chat history is not empty, you need to combine your answer with both evaluation and a new question:\n- first, you must use the evaluation tool by inputting both the User Input and Current Answer to evaluate the score and the correct answer. The format of the input is \"User input: <user input>; Current Answer: <current answer>\"\"\n- then, you must use the retrieval tool to get a question from the database\nThe format of the answer: \nThe score to the previous question is <the score to the answer> and the correct answer is <correct answer from the agent scratchpad>. <suggestions>. Please answer the next question. <question to be asked>\nAnswer: <answer to the next question>\n\n\nChat history:\n{chat_history}\n\nAgent Scratchpad:\n{agent_scratchpad}",
    "EVALUATION_TEMPLATE": "You are given a user input and a current answer.\n\nRate the human answer from 0 to 100, and then give human detailed suggestions on how to make their answer better according to the current answer given to you.\n\nIf the human's answer is \"I don't know\" or \"I don't understand\", or \"Base on my knowledge cut off, I do not have that information\", set the rating to be 0\nIf the human's answer is a question, check whether the question is relevant to the current answer. If it is not relevant, set the rating to be 0\nIf the human's statement is completely opposite to the correct answer, set the rating to be 0, and tell the human about this, give the human a suggestion on how to answer better\nIf the human's answer is largely related to the correct answer, set the rating to be 100\n\nIf the answer is correct, no need to give the correct answer.\nFormat:\nThe score to the previous question is <the score to the answer> out of 100. \nThe suggestion is <the suggestion for how to answer better>. \nThe correct answer is <current answer>",
    "SUMMARY_TEMPLATE": "You are given a chat history.\n\nGenerate an average score base on the AI messages, and then summarise how the human could have answered better by summarising the previous suggestions.\n\nYou need to give five scores: average score, context score, succintness score, detailness score, unsureness score\n\nYou must explain why you give this score base on the following rules:\nYou can calculate the in context score by checking how relevant the human's overall answer with regard to the correct answer/\nYou can check the detailness of the answer base on the length of the answer, and also how deeply the human has elaborated his answer. If the answer is a question, give the human a 0 mark.\nYou can calculate the succinctness of the answer base on the length of the answer, and also did the human repeat his sentences, that is, a concept appears over and over in one answer.\n\n\nFormat your answer in the following format:\nThe average score is <overall score>. The in context score is <in context score> out of 10. The succinctness score is <succintness> out of 5\nThe score of how detail the answer is <score> (out of 10). The bot was unsure about the answer for the following number of times <count how many times the human is unsure of his answer> (out of the total number of questions)",
    "CUDA_AVAILABLE": false,
    "USE_PRIVATE_TTS_API": true,
    "TRAINING_SIZE": 5,
    "GENERATE_TTS": false,
    "GPT_MODEL": "gpt-4"
}